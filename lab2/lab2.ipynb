{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f237fc-0634-4962-bf04-babeefd6bb9a",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Lab 2\n",
    "\n",
    "### Lab Date: Wednesday, February 5\n",
    "\n",
    "### Due: Wednesday, February 13\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Work with your lab group to complete the following notebook. It will be reviewed by your peers in lab next week (Wednesday, February 13th). \n",
    "\n",
    "This lab builds directly on your first lab. The idea is to use the pipeline you developed last week to solve two Bayesian decision theory problems. Both are optimal resource allocations problems. \n",
    "\n",
    "If you are new to working in python, or in a Jupyter notebook, please ask your lab members for help. If you notice a lab member struggling, and have experience, please offer your help.\n",
    "\n",
    "Please see this [Ed post](https://edstem.org/us/courses/74615/discussion/6107459) for corrections, questions, and discussion. If you would rather work with your own copy of the files, I have uploaded a zip folder there with the lab materials. You may use the same procedure you used last week to work with that folder. \n",
    "\n",
    "Corrections to the lab will be pushed directly to this notebook. We will only push corrections to the text, which is set to read only to prevent merge conflicts. In the event of a merge conflict, save your notebook under a different name, and click the link that launches the lab from the schedule on the [stat238 homepage](https://stat238.berkeley.edu/spring-2025/) again. Then, check for discrepancies. If you can't find them, or resolve the conflict, contact us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4b1c92-8982-47ea-9e1d-699fb2be367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load whatever packages you prefer here. I've added a reference list\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as scipy\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "# import pymc as pm\n",
    "# import bambi as bmb\n",
    "# import arviz as az\n",
    "# import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0fecc-ee45-4dcc-bba7-845ab6ed2609",
   "metadata": {
    "editable": false
   },
   "source": [
    "## The Setting\n",
    "\n",
    "This week's lab will use the ``Kidney Cancer\" data set from BDA (Gelman, Third Edition). The data set contains death counts due to kidney cancer in the United States. The counts are subdivided at a state and county level. The data set includes the county populations, and death counts over two 5 year periods (1980 - 1984, 1985 - 1989). \n",
    "\n",
    "Focus on the following columns in the kidney cancer dataset:\n",
    "* `state`: the US state\n",
    "* `Location`: the county and state as a string\n",
    "* `fips`, which provides the [FIPS code]() for each county: this is a standard identifier that can often be used to join datasets with county-level information.\n",
    "* `dc` and `dc.2`: the number of kidney cancer deaths between 1980-1984 and 1985-1989, respectively\n",
    "* `pop` and `pop.2`: the population between 1980-1984 and 1985-1989, respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecab235-e5fe-4502-aecf-ed84bb3c01ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>Location</th>\n",
       "      <th>dc</th>\n",
       "      <th>dc.2</th>\n",
       "      <th>pop</th>\n",
       "      <th>pop.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Autauga County, Alabama</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>61921</td>\n",
       "      <td>64915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Baldwin County, Alabama</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>170945</td>\n",
       "      <td>195253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Barbour County, Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33316</td>\n",
       "      <td>33987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Bibb County, Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30152</td>\n",
       "      <td>31175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Blount County, Alabama</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>88342</td>\n",
       "      <td>91547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state                 Location  dc  dc.2     pop   pop.2\n",
       "0  ALABAMA  Autauga County, Alabama   2     1   61921   64915\n",
       "1  ALABAMA  Baldwin County, Alabama   7    15  170945  195253\n",
       "2  ALABAMA  Barbour County, Alabama   0     1   33316   33987\n",
       "3  ALABAMA     Bibb County, Alabama   0     1   30152   31175\n",
       "4  ALABAMA   Blount County, Alabama   3     5   88342   91547"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc_full = pd.read_csv('kidney_cancer_1980.csv', skiprows=4)\n",
    "# There are many other interesting columns, but we'll focus on these:\n",
    "kc = kc_full.loc[:, ['state', 'Location', 'dc', 'dc.2', 'pop', 'pop.2']]\n",
    "kc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e930832-a63b-4fc4-98b3-633be7900951",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Take a moment with your group to explore this data set. You should notice that, outside of the specific context, it takes the same form as the synthetic data set you studied last lab. The synthetic data set consisted of a collection of the form $\\{n_j,S_j\\}_{j=1}^{100}$ where each $n_j$ was a number of draws of a Bernoulli random variable and $S_j$ was the number of succesful draws.\n",
    "\n",
    "Now, we have two datasets of this form. For each 5 year span you have a collection of the form $\\{(n_j(t),D_j(t))\\}_{j=1}^{2997}$. Here $t$ indexes a timespan (either 1980 - 1984 or 1985 - 1989)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ced937-8abe-44fd-8c01-49553612da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to perform whatever data exploration you want. \n",
    "# I suggest querying for the quantiles of the population and death count columns.\n",
    "# In particular, the min and max are important. \n",
    "# Notice also that, while the population sizes are large, the observed rate of deaths per individual are small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0766f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22382.5\n",
      "47628.0\n",
      "114626.25\n",
      "1.0\n",
      "2.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "print(np.quantile(kc['pop'], .25))\n",
    "print(np.quantile(kc['pop'], .5))\n",
    "print(np.quantile(kc['pop'], .75))\n",
    "\n",
    "print(np.quantile(kc['dc'], .25))\n",
    "print(np.quantile(kc['dc'], .5))\n",
    "print(np.quantile(kc['dc'], .75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1f737433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(kc['dc'], bins=25, range=(0, 50));\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(kc['dc.2'], bins=25, range=(0, 50));\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "90000c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22130.0\n",
      "48069.5\n",
      "117935.5\n",
      "1.0\n",
      "3.0\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "print(np.quantile(kc['pop.2'], .25))\n",
    "print(np.quantile(kc['pop.2'], .5))\n",
    "print(np.quantile(kc['pop.2'], .75))\n",
    "\n",
    "print(np.quantile(kc['dc.2'], .25))\n",
    "print(np.quantile(kc['dc.2'], .5))\n",
    "print(np.quantile(kc['dc.2'], .75))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fef6d-9f0c-45a5-876f-9e244b28e298",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Model\n",
    "\n",
    "We will work with the same model we used last week. This is, formally, our first example of a hierarchical model. A hierarchical model is simply a sampling model where the samples are performed sequentially, and where new samples may be sampled conditioned on the old samples.\n",
    "\n",
    "In particular, we will assume that, for each county and time frame (indexed by $j$ and $t$):\n",
    "\n",
    "1. **Background Parameters:** $\\alpha(t), \\beta(t)$. These are assumed to be fixed but unknown.\n",
    "1. **Death Rates:** $\\Theta_j(t) \\sim \\text{Beta}(\\alpha(t), \\beta(t))$.\n",
    "1. **Death Counts:** $D_j(t)|\\Theta_j(t) = \\theta \\sim \\text{Binomial}(n_j(t),\\theta)$.\n",
    "\n",
    "At each level of the hierarchy the set of samples are drawn i.i.d. \n",
    "\n",
    "Hierarchical models are helpful since they can be used to pool and share information. You practiced this last week when you used the first 99 draws to estimate the background parameters $\\alpha, \\beta$ that parameterized the prior needed for posterior inference on the 100th draw. In this lab, we will work with the full model. Here, sharing information is important since death rates due to kidney cancer are relatively low on a per capita basis. As a result, some small counties have very few observed deaths, even over a five-year period. Direct estimation of the death rates in those counties would be unreliable due to the small expected number of observed events in small counties. We will use the death rates observed in large counties to regularize our inference on small counties. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b5fe43-0989-4bc6-98b6-17abc27a6801",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Model Assumptions (A Warning)\n",
    "\n",
    "The model stated above makes a number of assumptions about the counties. Notice, for example, that it assumes that the prior distribution on the death rates is the same in all counties. This might not be the case, as counties with large populations tend to be urban (search for Cook County Illinois or Orange County California) while small counties tend to be rural. We might expect systematic differences in the death rates associated with county size that might break the exchangeability assumption used to ``borrow\" information from large counties when estimating death rates in small counties. We don't have the tools to treat this yet, but, it is worth remembering. The appropriate choice of Bayesian model should be informed by context knowledge. (Note: this is not too hard to check with a scatterplot. Try comparing county size to the MLE estimator for the death rate in each county... how concerned are you about our assumptions?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2124a494-f8bb-4828-b9b2-17e4652f5b3b",
   "metadata": {
    "editable": false
   },
   "source": [
    "## The Inference Pipeline\n",
    "\n",
    "In order to work with this model, we will need code to fit for the prior parameters, compute a posterior and perform posterior inference using the beta, binomial model, and to validate our models. Thankfully, you did all this already in Lab 1.\n",
    "\n",
    "Go back to Lab 1, and copy the functions you wrote there into the cells below. Change any variable names needed so that they can reference the data used in this lab.\n",
    "\n",
    "In this lab, we will focus on the first two stages of the BDA cycle, namely, fitting for the fixed prior parameters $\\alpha(t)$ and $\\beta(t)$ and deriving a posterior over the death rates conditioned on the observed death counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e97131c4-3c66-48e8-946d-d84e982ad6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_moms(s, n):\n",
    "    thetas = s/n\n",
    "    mean_est = np.mean(thetas)\n",
    "    var_est = np.var(thetas)\n",
    "    coeff = (((mean_est * (1 - mean_est)) / var_est) - 1)\n",
    "    alpha_mom = coeff * mean_est\n",
    "    beta_mom = coeff * (1 - mean_est)\n",
    "    return(alpha_mom, beta_mom)\n",
    "\n",
    "# Evidence Maximization (function from Reece)\n",
    "def evidence_maximization(s_samples, n_samples, initial_alpha, initial_beta): \n",
    "    from scipy.special import betaln, gammaln\n",
    "    def L2(params):\n",
    "        a, b = params\n",
    "        if a <= 0 or b <= 0:\n",
    "            return np.inf\n",
    "        log_terms = gammaln(n_samples + 1) - gammaln(s_samples + 1) - gammaln(n_samples - s_samples + 1) \\\n",
    "                    + betaln(s_samples + a, n_samples - s_samples + b) \\\n",
    "                    - betaln(a, b)\n",
    "        return -np.sum(log_terms)\n",
    "    result = scipy.optimize.minimize(L2, [initial_alpha, initial_beta], method='L-BFGS-B')\n",
    "    return result.x if result.success else (None, None)\n",
    "\n",
    "def predict_post_dist(s, n, alpha, beta):\n",
    "    from scipy.special import betaln, comb\n",
    "    log_prob = np.log(comb(n, s)) + betaln(s + alpha, n - s + beta) - betaln(alpha, beta)\n",
    "    return np.exp(log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "10e83fe2-405d-438d-a731-12c39f9dda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for performing posterior inference\n",
    "\n",
    "def calc_mle(s, n):\n",
    "    return s/n\n",
    "\n",
    "def calc_post_mean(s, n, a, b):\n",
    "    return (a + s)/(a + b + n)\n",
    "\n",
    "def calc_post_map(s, n, a, b):\n",
    "    return  (a + s - 1)/(a + b + n - 2)\n",
    "\n",
    "def calc_post_sd(s, n, a, b):\n",
    "    post_mean = calc_post_mean(s, n, a, b)\n",
    "    return np.sqrt((post_mean * (1 - post_mean))/(a + b + n + 1))\n",
    "\n",
    "def calc_cred_interval(a, b, p):\n",
    "    from scipy.stats import beta\n",
    "    upper_interval = beta.ppf((1 - p/2), a, b)\n",
    "    lower_interval = beta.ppf(p/2, a, b)\n",
    "    return lower_interval, upper_interval\n",
    "\n",
    "def get_post_params(s, n, a, b):\n",
    "    s0 = a\n",
    "    n0 = b + a\n",
    "    a_star = s + s0 + 1\n",
    "    b_star = n + n0 - (s + s0) + 1\n",
    "    return a_star, b_star\n",
    "\n",
    "def sample_post(a_star, b_star, m):\n",
    "    return np.random.beta(a_star, b_star, size=m)\n",
    "\n",
    "def plot_post_summary(a, b, m, p):\n",
    "    from scipy.stats import beta\n",
    "    samples = sample_post(a, b, m)\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    pdf_values = scipy.stats.beta.pdf(x, a, b)\n",
    "    lower_interval, upper_interval = calc_cred_interval(a, b, p)\n",
    "    post_mean = calc_post_mean(s, n, a, b)\n",
    "    post_map = calc_post_map(s, n, a, b)\n",
    "    mle = calc_mle(s, n)\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "    # Top plot: histogram and PDF\n",
    "    axs[0].hist(samples, bins=30, density=True, alpha=0.6, color='blue')\n",
    "    axs[0].plot(x, pdf_values, color='red')\n",
    "\n",
    "    # Set x and y limits for the top plot\n",
    "    axs[0].set_ylim(0, max(pdf_values))\n",
    "\n",
    "    # Bottom plot: posterior statistics with vertical lines\n",
    "    axs[1].axvline(x=post_mean, color='orange', linestyle='--', label=f'Posterior mean ({post_mean:.3f})')\n",
    "    axs[1].axvline(x=post_map, color='purple', linestyle='--', label=f'Posterior MAP ({post_map:.3f})')\n",
    "    axs[1].axvline(x=mle, color='black', linestyle='--', label=f'MLE ({mle:.3f})')\n",
    "    axs[1].axvline(x=lower_interval, color='green', linestyle='--', label=f'Lower credible interval ({lower_interval:.3f})')\n",
    "    axs[1].axvline(x=upper_interval, color='green', linestyle='--', label=f'Upper credible interval ({upper_interval:.3f})')\n",
    "\n",
    "    # Set x limits for the bottom plot\n",
    "    axs[1].set_xlim(-0.001, upper_interval + 0.01)\n",
    "\n",
    "    # Show the legend on the bottom plot\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Display the plots\n",
    "    plt.tight_layout()  # Adjust layout for better spacing\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4de00a92-efb7-4f03-a1f7-91d28d011de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for validating the model (this is optional)\n",
    "\n",
    "# Taiki's method: calculate credible interval for death counts posterior predictive estimate on each observation, check that true death counts is in the interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deec770-f8ef-47e0-8f3c-a7591e4770c4",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Fit for the Prior Parameters\n",
    "\n",
    "In the cell below, use the fitting procedure you developed in the last lab to get a guess at $\\alpha(t), \\beta(t)$ for the two time-spans. If your group did not build functioning code for this task last lab, either raise a hand to ask the TA for advice, or, talk to the other lab groups. You should have discussed strategies with those groups in your evaluations. You are welcome to try and implement any good ideas they described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "eef420b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S\n",
    "s1 = kc['dc']\n",
    "s2 = kc['dc.2']\n",
    "\n",
    "# N\n",
    "n1 = kc['pop']\n",
    "n2 = kc['pop.2']\n",
    "\n",
    "# # Filter low death counts\n",
    "# threshold = 3\n",
    "# keep1 = s1 >= threshold\n",
    "# keep2 = s2 >= threshold\n",
    "# s1 = s1[keep1]\n",
    "# n1 = n1[keep1]\n",
    "# s2 = s2[keep2]\n",
    "# n2 = n2[keep2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7d9e3cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3441068049306888 26641.11971852226\n",
      "1.3241508876353982 22886.723337046697\n"
     ]
    }
   ],
   "source": [
    "alpha_init1, beta_init1 = beta_moms(s1, n1)\n",
    "alpha_init2, beta_init2 = beta_moms(s2, n2)\n",
    "print(alpha_init1, beta_init1)\n",
    "print(alpha_init2, beta_init2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f70ab3ae-3c6f-4af3-9d7d-8227a228f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha1, beta1 = evidence_maximization(s1, s2, alpha_init1, beta_init1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92054e5-1b73-43e2-ae58-7e77437ba329",
   "metadata": {
    "editable": false
   },
   "source": [
    "Clearly report your estimates for $\\alpha(t), \\beta(t)$. \n",
    "- What do you notice about these parameters?\n",
    "- What do they imply about the shape of your prior?\n",
    "- How does the prior mean compare to the MLE for the national death rate due to kidney cancer? How does it compare to the MLE among just the largest counties (say, the top twenty)? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c19de8-6802-46fe-8424-fceab071bcbb",
   "metadata": {},
   "source": [
    "*Fill in your answers here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c20c6-c6f6-4b50-8ab4-bf7e3717a440",
   "metadata": {
    "editable": false
   },
   "source": [
    "### Infer Death Rates\n",
    "\n",
    "Pick three different counties from the data set and form posterior estimates for their death rates in each of the two time periods. Select a small county, a large county, and a county of intermediate size (say, size close to the median size over all counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b0a3fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def county_by_quantile(df, percentile_value):\n",
    "    df['mean_pop'] = df[['pop', 'pop.2']].mean(axis=1)\n",
    "    percentile_pop = np.quantile(df['mean_pop'], percentile_value)\n",
    "    df_sorted = df.sort_values(by='mean_pop', ascending=False)\n",
    "    county = df_sorted[df_sorted['mean_pop'] <= percentile_pop]['Location'].iloc[0]\n",
    "    return df.loc[df['Location'] == county]\n",
    "\n",
    "def get_obs(df, timeframe=1):\n",
    "    dc = \"dc\"\n",
    "    pop = \"pop\"\n",
    "    if timeframe == 2:\n",
    "        dc = \"dc.2\"\n",
    "        pop = \"pop.2\"\n",
    "    return df[dc].iloc[0], df[pop].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f00489d1-a79a-413c-8e84-75d3b905d90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9.218945625224804e-06, 0.0001585542584770637)\n",
      "(9.477158705010159e-06, 0.0001629948574692387)\n"
     ]
    }
   ],
   "source": [
    "# Perform posterior inference\n",
    "\n",
    "# Small county\n",
    "# Time period 1\n",
    "percentile_value = .1\n",
    "df = county_by_quantile(kc, percentile_value)\n",
    "s, n = get_obs(df)\n",
    "alpha_star, beta_star = get_post_params(alpha_init1, beta_init1, s, n)\n",
    "# plot_post_summary(alpha_star, beta_star, m=1000, p=.05)\n",
    "print(calc_cred_interval(alpha_star, beta_star, p))\n",
    "\n",
    "# Time period 2\n",
    "df = county_by_quantile(kc, percentile_value)\n",
    "s, n = get_obs(df, 2)\n",
    "alpha_star, beta_star = get_post_params(alpha_init1, beta_init1, s, n)\n",
    "# plot_post_summary(alpha_star, beta_star, m=1000, p=.05)\n",
    "print(calc_cred_interval(alpha_star, beta_star, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "76cd7d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.218945625224804e-06 0.0001585542584770637\n",
      "9.477158705010159e-06 0.0001629948574692387\n"
     ]
    }
   ],
   "source": [
    "# Perform posterior inference for two time periods\n",
    "\n",
    "# Small county\n",
    "percentile_value = 0.1\n",
    "p = 0.05\n",
    "\n",
    "# Time period 1\n",
    "df1 = county_by_quantile(kc, percentile_value)\n",
    "s1, n1 = likelihood_params(df1)\n",
    "alpha_star1, beta_star1 = get_post_params(alpha_init1, beta_init1, s1, n1)\n",
    "lower_bounds, upper_bounds = calc_cred_interval(alpha_star1, beta_star1, p)\n",
    "print(lower_bounds, upper_bounds)\n",
    "\n",
    "# Time period 2\n",
    "df2 = county_by_quantile(kc, percentile_value)\n",
    "s2, n2 = likelihood_params(df2, 2)\n",
    "alpha_star2, beta_star2 = get_post_params(alpha_init1, beta_init1, s2, n2)\n",
    "lower_bounds, upper_bounds = calc_cred_interval(alpha_star2, beta_star2, p)\n",
    "print(lower_bounds, upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "21c2d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.7303751518361542e-05, 0.000126692399761805)\n",
      "(4.720249165147601e-06, 8.118530128215483e-05)\n"
     ]
    }
   ],
   "source": [
    "# Medium county\n",
    "\n",
    "# Time period 1\n",
    "percentile_value = .5\n",
    "df = county_by_quantile(kc, percentile_value)\n",
    "s, n = likelihood_params(df)\n",
    "alpha_star, beta_star = get_post_params(alpha_init1, beta_init1, s, n)\n",
    "# plot_post_summary(alpha_star, beta_star, m=1000, p=.05)\n",
    "print(calc_cred_interval(alpha_star, beta_star, p))\n",
    "\n",
    "# Time period 2\n",
    "df = county_by_quantile(kc, percentile_value)\n",
    "s, n = likelihood_params(df, 2)\n",
    "alpha_star, beta_star = get_post_params(alpha_init1, beta_init1, s, n)\n",
    "# plot_post_summary(alpha_star, beta_star, m=1000, p=.05)\n",
    "print(calc_cred_interval(alpha_star, beta_star, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7b095f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.1388146551717815e-05, 7.93604653715291e-05)\n",
      "(2.5758574264003117e-05, 7.121073594430213e-05)\n"
     ]
    }
   ],
   "source": [
    "# Large county\n",
    "\n",
    "# Time period 1\n",
    "percentile_value = .9\n",
    "df = county_by_quantile(kc, percentile_value)\n",
    "s, n = likelihood_params(df)\n",
    "alpha_star, beta_star = get_post_params(alpha_init1, beta_init1, s, n)\n",
    "# plot_post_summary(alpha_star, beta_star, m=1000, p=.05)\n",
    "print(calc_cred_interval(alpha_star, beta_star, p))\n",
    "\n",
    "# Time period 2\n",
    "df = county_by_quantile(kc, percentile_value)\n",
    "s, n = likelihood_params(df, 2)\n",
    "alpha_star, beta_star = get_post_params(alpha_init1, beta_init1, s, n)\n",
    "# plot_post_summary(alpha_star, beta_star, m=1000, p=.05)\n",
    "print(calc_cred_interval(alpha_star, beta_star, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ad757-6dfb-40f3-aa6f-3756e05eaf73",
   "metadata": {
    "editable": false
   },
   "source": [
    "In the space below: \n",
    "1. Report an interval estimate for the true death rates in each of your selected counties given the observed death counts over each time period. Select the intervals so that they should contain the true value with at least a 95% chance.\n",
    "1. Comment on the difference in the width of the intervals between the large and the small counties.\n",
    "1. Compare the interval estimate to the MLE in each county. Which counties have it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9fbad-5781-42af-ab2a-08247438d55e",
   "metadata": {},
   "source": [
    "*Fill in your answers here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9af73c-2d6d-47ce-9c13-98c86092a4f8",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Decision Theory\n",
    "\n",
    "Imagine that you work for the NIH, and have been asked to advise the federal government in an effort to distribute research and treatment funds to US counties. For the sake of this exercise, imagine that you are performing this task at the end of the survey that ran from 1980 - 1984. Your resource allocation problems are:\n",
    "\n",
    "1. Given a fixed total budget to be spent on treatment programs, distributed at a county level, how should treatment spending be distributed amongst the counties to best protect the American public?\n",
    "1. Given a fixed total research budget, where should the NIH send targeted survey teams to maximize the amount of information they expect to gain from a follow-up survey?\n",
    "\n",
    "These two scenarios are explained in detail below. (*Note: the following scenarios are stylized to make the subsequent exercise feasible.*)\n",
    "\n",
    "**Scenario 1: Treatment Allocation** Suppose that, as part of the war on cancer, the federal government is considering allocating 140 million dollars per year to kidney cancer treatment programs. These funds will be allocated at the county level. Adjusted for inflation (adjusted to 1985), the average cost of kidney cancer treatment in fatal cases would have been roughly 35,000 dollars. The proposed total allocation is close to, but not quite, enough to cover the cost of each fatal kidney cancer case recorded in the 1980 - 1985 period (the total necessary cost would have been 163.9 million). \n",
    "\n",
    "You are asked to give an initial suggestion for how to distribute these funds across the counties based on your analysis of the available data. To start, you pose the following optimization problem:\n",
    "\n",
    "1. **Allocation:** You represent your allocation as a distribution vector with elements $\\{a_j\\}_{j=1}^{2997}$, where $a_j$ represents the fraction of the total allocation you would allocate to county $j$.\n",
    "2. **Loss (per county):** You aim to allocate enough funds to each county so that, the money sent to each county could cover the treatment cost for each fatal case in the county in the coming year. Let $D_j(t')$ represent the number of potentially fatal cases you hope to treat in the next year, $t'$. If $D_j(t') = d_j$ and you allocated $a_j \\times 140 \\times 10^6$ dollars to county $j$, then you compute your loss piecewise as:\n",
    "    $$l_j(d_j,a_j) = \\min\\{d_j \\times 35 \\times 10^3 - a_j \\times 140 \\times 10^6, 0\\} $$\n",
    "   Then, your loss is equal to the total shortfall in treatment funding you assigned to the $j^{th}$ county. Since the federal funding is not enough to cover all counties, you do not add a penalty for over-allocating to a county (*this will be accounted for by trying to distribute limited funds to minimize the shortfall across all counties. If you'd like to try adding a cost for overbudgeting, add a separate loss term that is proportional to $a_j \\times 140 \\times 10^6 -  d_j \\times 35 \\times 10^3$ when this difference is positive. To prioritize treatment over efficiency, you should weight this term by some factor less than one.*) \n",
    "3. **Total Loss:** You define your total loss, given $\\{D_j(t') = d_j, a_j\\}_{j=1}^{2997}$, as the total loss over all counties:\n",
    "   $$l(d,a) = \\sum_{j=1}^{2997} l_j(d_j,a_j) $$\n",
    "5. **Bayes Risk:** You define your Bayesian risk, conditioned on the observed data, to be equal to your expected loss, given an allocation $a$, and given your past data:\n",
    "    $$\\begin{aligned} r(a|D(t) = d) & = \\mathbb{E}_{D(t')|D(t) = d}[l(D(t'),a)] \\\\ & = \\sum_{j=1}^{2997} \\mathbb{E}_{D_j(t')|D_j(t) = d_j}[l_j(D_j(t'),a_j)] \\end{aligned}$$\n",
    "   Note that each expectation runs over the *posterior predictive* distribution for the number of fatal kidney cancer cases in 1 new year, given the data collected over the past 5.\n",
    "6. You aim to select the allocation that minimizes your Bayesian risk:\n",
    "   $$a_*(d) = \\text{argmin}_{a}\\{r(a|D(t) = d)\\} $$\n",
    "\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "f937bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_map(alpha_init, beta_init, s, n):\n",
    "    alpha_star, beta_star = get_post_params(alpha_init, beta_init, s, n)\n",
    "    return calc_post_map(s, n, alpha_star, beta_star)\n",
    "\n",
    "def get_upper_cred_interval(alpha_init, beta_init, s, n, p):\n",
    "    alpha_star, beta_star = get_post_params(alpha_init, beta_init, s, n)\n",
    "    return calc_cred_interval(alpha_star, beta_star, p)[1]  # Upper bound of interval\n",
    "\n",
    "# Compute allocation factor using MAP estimate\n",
    "def map_allocation_factor(df, alpha_init, beta_init):\n",
    "    df['post_map'] = df.apply(lambda row: \n",
    "            get_post_map(alpha_init, beta_init, \n",
    "                         row['death_counts'], row['population']), \n",
    "        axis=1)\n",
    "    map_af = df['post_map'] * df['population']\n",
    "    df['map_af'] = map_af / map_af.sum()\n",
    "    return df\n",
    "\n",
    "# Compute allocation factor using upper credible interval\n",
    "def credible_allocation_factor(df, alpha_init, beta_init, p):\n",
    "    df['upper_interval'] = df.apply(lambda row: \n",
    "            get_upper_cred_interval(alpha_init, beta_init, \n",
    "                                    row['death_counts'], row['population'], \n",
    "                                    p), \n",
    "        axis=1)\n",
    "    credible_af = df['upper_interval'] * df['population']\n",
    "    df['credible_af'] = credible_af / credible_af.sum()\n",
    "    return df\n",
    "\n",
    "def calc_bayes_risk(df, alpha_init, beta_init, \n",
    "                    allocation_factor, \n",
    "                    n_resamples=100):\n",
    "    \n",
    "    def calc_expected_county_loss(s, n, allocation_factor):\n",
    "        from scipy.stats import betabinom\n",
    "        \n",
    "        alpha_star, beta_star = get_post_params(alpha_init, beta_init, s, n)\n",
    "        \n",
    "        # Sample new cases from current county\n",
    "        # np.random.seed(666)\n",
    "        new_cases = betabinom.rvs(n, alpha_star, beta_star, size=n_resamples)\n",
    "        \n",
    "        a_j = allocation_factor * 140 * 10**6 # Allocated budget\n",
    "        d_j = new_cases * 35 * 10**3 # Treatment cost\n",
    "        \n",
    "        return np.mean(np.maximum(d_j - a_j, 0)) # Average shortfall\n",
    "\n",
    "    # Compute expected loss for each county\n",
    "    df['mean_county_loss'] = df.apply(lambda row: \n",
    "            calc_expected_county_loss(row['death_counts'], \n",
    "                                      row['population'], \n",
    "                                      row[allocation_factor]), \n",
    "        axis=1)\n",
    "\n",
    "    # Return total Bayes risk\n",
    "    return df['mean_county_loss'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb0205-4148-4b7d-b3c1-335c0f5d0fa6",
   "metadata": {
    "editable": false
   },
   "source": [
    "This problem is too big to optimize easily (2997 variables! Noisy evaluation!). So, instead of trying to optimize it exactly, let's try some heuristic allocation rules, and select the one with the least risk.\n",
    "\n",
    "Implement the heuristic allocation procedures suggested below then select the one with the smallest risk:\n",
    "1. Set the allocation across the counties proportional to (the MAP estimators for the death rates) $\\times$ (the respective county populations)\n",
    "1. Set the allocation across the counties proportional to (the $p$% upper credible bound on the death rates) $\\times$ (the respective county populations). The $p$% upper credible bound on the rate in county $j$ is the value ${\\theta_u}_j(p)$ such that $\\text{Pr}(\\Theta_j(t) \\leq {\\theta_u}_j(p)) = p$. If you wrote a code last week to produce credible intervals based on the CDF of the posterior, you've already written the code to evaluate this. Try varying $p$.\n",
    "\n",
    "*Hint: for an alternative heuristic, think about whether the loss function above is more directly related to the posterior over the unknown rates, or the posterior predictive distribution. If you'd rather try a heuristic based on the posterior predictive distribution, go ahead!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "1ec6d1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3120409.855880618\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(666)\n",
    "\n",
    "p = .05\n",
    "\n",
    "df = kc\n",
    "df['death_counts'] = df['dc']\n",
    "df['population'] = df['pop']\n",
    "\n",
    "alpha_init = alpha_init1\n",
    "beta_init = beta_init1\n",
    "\n",
    "df = credible_allocation_factor(df, alpha_init, beta_init, p)\n",
    "df = map_allocation_factor(df, alpha_init, beta_init)\n",
    "\n",
    "credible_risk = calc_bayes_risk(df, alpha_init1, beta_init2, allocation_factor=\"credible_af\", n_resamples=100)\n",
    "map_risk = calc_bayes_risk(df, alpha_init1, beta_init2, allocation_factor=\"map_af\", n_resamples=100)\n",
    "\n",
    "print(credible_risk - map_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06e076-7893-4df7-b595-b2734f1ae4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) try changing the total allocation. Make it much smaller than\n",
    "# the value needed to cover all cases on average (say, 60 million)\n",
    "# try making it much larger\n",
    "# does your best allocation strategy change in a sensible way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c246a0-a5ce-461e-81d5-39df1ad4dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) if you like thorny optimization problems, try passing this to\n",
    "# an optimization scheme and see if you can solve for an allocation that beats \n",
    "# your best heuristic. It may help to warm-up on a single state before trying the \n",
    "# federal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8328e-7aa5-40d8-961e-44ae0af70847",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Scenario 2: Research Allocation**\n",
    "\n",
    "Suppose, instead, that the NIH intends to run a follow-up survey that can screen a sample of up to 25,000 individuals drawn from a given county. The NIH wants to pick a few candidate counties to survey. They hope to pick counties such that, the effort and money spent on the survey are expected to lead to the greatest gain in knowledge. \n",
    "\n",
    "This is an example of a Bayesian optimal search or optimal experimental design question. Given a set of available measurements, which should I pick to maximize the amount I expect to learn from the measurement? \n",
    "\n",
    "One way to frame these problems is to define a utility function, or information acquisition function, which measures how much we would learn after an observation. Then, we might try to maximize the expected utility over all possible observations given the choice of measurement. (*Near-to-optimal search trees are often constructed by greedy iteration of this procedure. The tree terminates when the unknown is determined within a chosen uncertainty. In cases when the measurements return answers drawn from a finite set, the sequence of answers to the sequence of questions form a string that points to a conclusion, thus constructs an encoding. These encodings are typically close to information theoretic optimal in the sense that they are maximally efficient (require as few questions as possible, in expectation, to reach a conclusion). They are a classical tool of statistical data compression.*)\n",
    "\n",
    "The decision problem can be posed as follows:\n",
    "\n",
    "1. You get to choose which county to survey. Your space of available actions is the set of counties.\n",
    "1. Consider two different criteria for selecting a county:\n",
    "   1. Pick the county where you have the most opportunity to learn from observations. That is, choose the county, $j_*$ for which, you would expect the greatest reduction in posterior standard deviation in the death rate, if you had access to another 5 year record from that county among, **at most**, a sample population of 25,000 individuals drawn county $j_*$. (*This assumes that the survey data produced by screening can provide the equivalent information content as an additional five year sample, and that after converting to a count, is identically distributed to actual count data. In other words, the survey data can be used to produce predictions which are an accurate proxy for real events. This is to establish a simple heuristic. We'll see better motivated examples later in the class.*)\n",
    "      That is, select the county:\n",
    "     $$j_* = \\text{argmax}_{j|n_j}\\left\\{\\mathbb{E}_{D_j(t')|d_j(t)}\\left[\\text{SD}[\\Theta_j|d_j] - \\text{SD}[\\Theta_j|d_j(t),D_j(t')]\\right] \\right\\} $$\n",
    "      As usual, other reasonable definitions are possible here. (*For example, we could pick the county where we expect the greatest reduction in variance, greatest reduction of coefficient of variation, or, to approach information-theoretic optimality, the greatest reduction in entropy.*)\n",
    "\n",
    "   1. Pick the county that could plausibly have the largest death rate. This choice could be motivated if our aim is to identify causes of kidney cancer mortality. In particular, let's say that a county could plausibly admit a death rate of, at most ${\\theta_u}_j$ if ${\\theta_u}_j$ acts as an upper credible bound on the unknown. That is, if $\\Theta_j \\leq {\\theta_u}_j$ with high probability. For a 90% upper credible interval, we need to find a bound ${\\theta_u}_j$ such that $\\text{Pr}(\\Theta_j \\leq {\\theta_u}_j|d_j(t)) \\geq 0.90$. To find such a bound, use the analytic CDF of the posterior on each county, or estimate it by sampling from the posterior. You should have written the code to perform one of these two procedures last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec76308-ac14-4ba9-a43b-d64571a20ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a code here to compute the expected reduction in posterior standard deviation in each county\n",
    "# if we drew 5 new years worth of data for max{n_j,25000} individuals\n",
    "\n",
    "# You can look up the variance in the beta-binomial, or check your work in HW 1\n",
    "\n",
    "# Then, simply evaluate the expected reduction on every county, and pick the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f85a6-c1be-4ca0-926b-64c4f565cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a code here to compute the 90% upper credible bound on the death rate in each county\n",
    "# return the county with the largest 90% upper credible bound"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
